{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "class SampleGenerator(object):\n",
    "    \"\"\"Construct dataset for multi-class classification\"\"\"\n",
    "\n",
    "    def __init__(self, ratings, n_test=1):\n",
    "        \"\"\"\n",
    "        args:\n",
    "            ratings: pd.DataFrame, which contains 4 columns = ['userId', 'itemId', 'rating', 'timestamp']\n",
    "        \"\"\"\n",
    "        assert 'userId' in ratings.columns\n",
    "        assert 'itemId' in ratings.columns\n",
    "        assert 'rating' in ratings.columns\n",
    "\n",
    "        self.ratings = ratings\n",
    "        # self.preprocessed_ratings = self._preprocess_ratings(ratings)\n",
    "        self.train_ratings, self.test_ratings = self._split_loo(self.ratings, n_test)\n",
    "\n",
    "    def _split_loo(self, ratings, n_test=1):\n",
    "        \"\"\"\n",
    "        Split dataset into train and test sets, with `n_test` interactions per user in the test set.\n",
    "\n",
    "        Args:\n",
    "            ratings: DataFrame, contains user-item interactions.\n",
    "            n_test: int, number of recent interactions to include in the test set.\n",
    "\n",
    "        Returns:\n",
    "            train: DataFrame, contains the train set.\n",
    "            test: DataFrame, contains the test set.\n",
    "        \"\"\"\n",
    "        # Rank interactions by timestamp for each user\n",
    "        ratings['rank_latest'] = ratings.groupby(['userId'])['timestamp'].rank(method='first', ascending=False)\n",
    "        \n",
    "        # Test set contains the top `n_test` interactions for each user\n",
    "        test = ratings[ratings['rank_latest'] <= n_test]\n",
    "        \n",
    "        # Train set contains all other interactions\n",
    "        train = ratings[ratings['rank_latest'] > n_test]\n",
    "\n",
    "        print(train['userId'].nunique())\n",
    "        print(test['userId'].nunique())\n",
    "        # Ensure every user is in both train and test sets\n",
    "        # assert train['userId'].nunique() == test['userId'].nunique()\n",
    "\n",
    "        # Return train and test dataframes\n",
    "        return train[['userId', 'itemId', 'rating']], test[['userId', 'itemId', 'rating']]\n",
    "\n",
    "    def evaluate_data(self):\n",
    "        \"\"\"Create evaluate data for classification\"\"\"\n",
    "        test_users = self.test_ratings['userId'].tolist()\n",
    "        test_items = self.test_ratings['itemId'].tolist()\n",
    "        test_ratings = self.test_ratings['rating'].tolist()\n",
    "\n",
    "        return [torch.LongTensor(test_users), torch.LongTensor(test_items), torch.LongTensor(test_ratings)]\n",
    "    \n",
    "    def get_train_test_dataframes(self):\n",
    "        \"\"\"\n",
    "        Returns the train and test dataframes.\n",
    "        \"\"\"\n",
    "        return self.train_ratings, self.test_ratings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Range of userId is [0, 226569]\n",
      "Range of itemId is [0, 231636]\n",
      "37576\n",
      "226570\n",
      "(845483, 3) (286884, 3)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "\n",
    "# Load Data\n",
    "data_dir = '/raid/home/shyamsg/Final_Project/DecoderOnlyModel/Data/food.dat'\n",
    "rating_df = pd.read_csv(data_dir, sep='\\t', header=None, names=['uid', 'mid', 'rating', 'timestamp'], engine='python')\n",
    "\n",
    "\n",
    "# Reindex userId and itemId, start from 0\n",
    "user_id = rating_df[['uid']].drop_duplicates().reindex()\n",
    "user_id['userId'] = np.arange(len(user_id))\n",
    "rating_df = pd.merge(rating_df, user_id, on=['uid'], how='left')\n",
    "\n",
    "# Reindex itemId, start from 0\n",
    "item_id = rating_df[['mid']].drop_duplicates()\n",
    "item_id['itemId'] = np.arange(len(item_id))\n",
    "rating_df = pd.merge(rating_df, item_id, on=['mid'], how='left')\n",
    "\n",
    "rating_df = rating_df[['userId', 'itemId', 'rating', 'timestamp']]\n",
    "\n",
    "# i want to change 0 ratings to 1\n",
    "rating_df['rating'] = rating_df['rating'].apply(lambda x: 1 if x == 0 else x)\n",
    "\n",
    "print('Range of userId is [{}, {}]'.format(rating_df.userId.min(), rating_df.userId.max()))\n",
    "print('Range of itemId is [{}, {}]'.format(rating_df.itemId.min(), rating_df.itemId.max()))\n",
    "\n",
    "\n",
    "# n_test specifies number of most recent interactions to be used for testing for each user\n",
    "sample_generator = SampleGenerator(rating_df, n_test=2)\n",
    "\n",
    "# Get train and test dataframes\n",
    "df1, df2 = sample_generator.get_train_test_dataframes()\n",
    "\n",
    "print(df1.shape, df2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(845483, 2)\n",
      "(286884, 2)\n",
      "Train Size: 845483, Val Size: 286884\n"
     ]
    }
   ],
   "source": [
    "# now create a new dataframe columns\n",
    "#df['prompt'] = What star rating do you think userid_df[user_id] will give item_df[item_id]? \n",
    "#df['rating'] = df[rating]\n",
    "\n",
    "train_df = pd.DataFrame()\n",
    "train_df['prompt'] = \"What star rating do you think user_\" + df1['userId'].astype(str) + \" will give item_\" + df1['itemId'].astype(str) + \"?\"\n",
    "train_df['rating'] = df1['rating']\n",
    "print(train_df.shape)   \n",
    "\n",
    "val_df = pd.DataFrame()\n",
    "val_df['prompt'] = \"What star rating do you think user_\" + df2['userId'].astype(str) + \" will give item_\" + df2['itemId'].astype(str) + \"?\"\n",
    "val_df['rating'] = df2['rating']\n",
    "print(val_df.shape)\n",
    "\n",
    "print(f\"Train Size: {len(train_df)}, Val Size: {len(val_df)}\")\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid/home/shyamsg/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Full definition of a GPT Language Model, all of it in this single file.\n",
    "References:\n",
    "1) the official GPT-2 TensorFlow implementation released by OpenAI:\n",
    "https://github.com/openai/gpt-2/blob/master/src/model.py\n",
    "2) huggingface/transformers PyTorch implementation:\n",
    "https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py\n",
    "Code adopted from: https://github.com/karpathy/nanoGPT\n",
    "\"\"\"\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from dataclasses import dataclass\n",
    "from transformers import GPT2LMHeadModel, GPT2ForSequenceClassification\n",
    "\n",
    "\n",
    "################################################################################\n",
    "# GPT Model classes\n",
    "################################################################################\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024\n",
    "    vocab_size: int = 50304\n",
    "    n_layer: int = 12\n",
    "    n_head: int = 12\n",
    "    n_embd: int = 768\n",
    "    dropout: float = 0.0\n",
    "    bias: bool = True\n",
    "\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    \"\"\" LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False \"\"\"\n",
    "    def __init__(self, ndim, bias):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(ndim))\n",
    "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
    "\n",
    "    def forward(self, input):\n",
    "        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)\n",
    "\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        # regularization\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.dropout = config.dropout\n",
    "        # flash attention, but support is only in PyTorch >= 2.0\n",
    "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
    "        if not self.flash:\n",
    "            # causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "            self.register_buffer(\"bias\", torch.tril(\n",
    "                torch.ones(config.block_size, config.block_size))\n",
    "                .view(1, 1, config.block_size, config.block_size))\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "\n",
    "\n",
    "        attn_out = self.c_attn(x)\n",
    "\n",
    "        # TODO: send x through the auxiliary c_attn and add them back to attn_out\n",
    "        # x = self.c_attn_GPT2model(x)\n",
    "        # attn_out = attn_out + x\n",
    "        # print(f\"shape of x in CausalSelfAttention: {attn_out.shape}\")\n",
    "\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        q, k, v  = attn_out.split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        if self.flash:\n",
    "            # efficient attention using Flash Attention CUDA kernels\n",
    "            y = torch.nn.functional.scaled_dot_product_attention(\n",
    "                q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)\n",
    "        else:\n",
    "            # manual implementation of attention\n",
    "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "            att = F.softmax(att, dim=-1)\n",
    "            att = self.attn_dropout(att)\n",
    "            y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        proj_out = self.c_proj(y)\n",
    "\n",
    "        # TODO: send y through the auxiliary c_proj and add them back to proj_out\n",
    "        # added by me\n",
    "        # proj_out = proj_out + self.c_proj_GPT2model(y)\n",
    "        # added by me \n",
    "\n",
    "        y = self.resid_dropout(proj_out)\n",
    "        \n",
    "        # print(f\"shape of y after residual dropout: {y.shape}\")\n",
    "        return y\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
    "        self.gelu    = nn.GELU()\n",
    "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "        # added by me \n",
    "\n",
    "    def forward(self, x):\n",
    "        # commented by me which they have given\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)\n",
    "    \n",
    "        return x\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, model_type='gpt2', is_gen=False):\n",
    "        super(GPT, self).__init__()\n",
    "        \n",
    "        # n_layer, n_head and n_embd are determined from model_type\n",
    "        config_args = {\n",
    "            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n",
    "            'gpt2-small':  dict(n_layer=12, n_head=12, n_embd=768), # later last 6 layers will be removed\n",
    "            # 'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n",
    "            # 'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n",
    "            # 'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M (1.3B) params\n",
    "        }[model_type]\n",
    "        print(\"forcing vocab_size=50257, block_size=1024, bias=True\")\n",
    "        config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints\n",
    "        config_args['block_size'] = 1024  # always 1024 for GPT model checkpoints\n",
    "        config_args['bias'] = True        # always True for GPT model checkpoints\n",
    "\n",
    "        self.config = GPTConfig(**config_args)\n",
    "        \n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(self.config.vocab_size, self.config.n_embd),\n",
    "            wpe = nn.Embedding(self.config.block_size, self.config.n_embd),\n",
    "            drop = nn.Dropout(self.config.dropout),\n",
    "            h = nn.ModuleList([Block(self.config) for _ in range(self.config.n_layer)]),\n",
    "            ln_f = LayerNorm(self.config.n_embd, bias=self.config.bias),\n",
    "        ))\n",
    "        if is_gen:\n",
    "            self.lm_head = nn.Linear(\n",
    "                self.config.n_embd, self.config.vocab_size, bias=False)\n",
    "            self.transformer.wte.weight = self.lm_head.weight  # https://paperswithcode.com/method/weight-tying\n",
    "            self.score = None\n",
    "        else:\n",
    "            self.score = nn.Linear(self.config.n_embd, 5, bias=False)\n",
    "\n",
    "        # TODO: Remove gradients from the embedding layers\n",
    "        # added by me below\n",
    "        self.transformer.wte.weight.requires_grad = False\n",
    "        self.transformer.wpe.weight.requires_grad = False\n",
    "        # added by me above \n",
    "\n",
    "        sd = self.state_dict()\n",
    "        sd_keys = sd.keys()\n",
    "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')]\n",
    "\n",
    "        # init a huggingface/transformers model\n",
    "        print(\"loading weights from pretrained gpt2 model\")\n",
    "        if is_gen:\n",
    "            model_hf = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "        else:\n",
    "            model_hf = GPT2ForSequenceClassification.from_pretrained('gpt2')\n",
    "\n",
    "        sd_hf = model_hf.state_dict()\n",
    "\n",
    "        # copy while ensuring all of the parameters are aligned and match in names and shapes\n",
    "        sd_keys_hf = sd_hf.keys()\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')]\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')]\n",
    "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
    "        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
    "        # intialize the score.weight with xavier initialization \n",
    "        for k in sd_keys:\n",
    "            if k.endswith('score.weight'):\n",
    "                nn.init.xavier_uniform_(sd[k])\n",
    "        # remove the score.weigth from the list of keys\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('score.weight')]\n",
    "        for k in sd_keys_hf:\n",
    "            if any(k.endswith(w) for w in transposed):\n",
    "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k].t())\n",
    "            else:\n",
    "                # vanilla copy over the other parameters except the params that are not in the pretrained model\n",
    "                assert sd_hf[k].shape == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k])\n",
    "                    \n",
    "        # Remove the last 6 layers for gpt2-small variant\n",
    "        if model_type == 'gpt2-small':\n",
    "            self.transformer.h = self.transformer.h[:6]\n",
    "                    \n",
    "        # print the total number of parameters\n",
    "        total_params = sum(p.numel() for p in self.parameters())\n",
    "        print(f\"Number of parameters: {total_params / 1e6:.2f}M\")\n",
    "        if not is_gen:\n",
    "            # print the number of trainable parameters\n",
    "            num_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "            print(f\"Number of trainable parameters: {num_params / 1e6:.2f}M\")\n",
    "            # calculate the reduction in parameters\n",
    "            reduction = 100 * (total_params - num_params) / total_params\n",
    "            print(f\"Reduction: {reduction:.2f}%\")\n",
    "\n",
    "    def forward(self, idx, mask=None):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device)\n",
    "\n",
    "        if mask is not None:\n",
    "            eos_idxs = mask.sum(1) - 1\n",
    "\n",
    "        tok_emb = self.transformer.wte(idx)\n",
    "        pos_emb = self.transformer.wpe(pos)\n",
    "        x = self.transformer.drop(tok_emb + pos_emb)\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "\n",
    "        if self.score is not None:\n",
    "            if mask is not None:\n",
    "                logits = self.score(x[torch.arange(b, device=device), eos_idxs])\n",
    "            else:\n",
    "                logits = self.score(x[:, -1, :])\n",
    "        else:\n",
    "            logits = self.lm_head(x[:, [-1], :])\n",
    "\n",
    "        # return logits.squeeze(-1)  # Ensure shape (batch_size,)\n",
    "        return logits\n",
    "\n",
    "    def crop_block_size(self, block_size):\n",
    "        # model surgery to decrease the block size if necessary\n",
    "        # e.g. we may load the GPT2 pretrained model checkpoint (block size 1024)\n",
    "        # but want to use a smaller block size for some smaller, simpler model\n",
    "        assert block_size <= self.config.block_size\n",
    "        self.config.block_size = block_size\n",
    "        self.transformer.wpe.weight = nn.Parameter(self.transformer.wpe.weight[:block_size])\n",
    "        for block in self.transformer.h:\n",
    "            if hasattr(block.attn, 'bias'):\n",
    "                block.attn.bias = block.attn.bias[:,:,:block_size,:block_size]\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        \"\"\"\n",
    "        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
    "        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
    "        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
    "        \"\"\"\n",
    "        \n",
    "       \n",
    "        for _ in range(max_new_tokens):\n",
    "            # if the sequence context is growing too long we must crop it at block_size\n",
    "            \n",
    "            # print(f\"shape of idx in generate: {idx.shape}\") # added by me \n",
    "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
    "            # forward the model to get the logits for the index in the sequence\n",
    "            logits = self(idx_cond)\n",
    "            # pluck the logits at the final step and scale by desired temperature\n",
    "            \n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            # optionally crop the logits to only the top k options\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "            # apply softmax to convert logits to (normalized) probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            # append sampled index to the running sequence and continue\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n",
    "    \n",
    "    def save_trainable_params(self, path):\n",
    "        trainable_params =\\\n",
    "            list(filter(lambda p: p.requires_grad, self.parameters()))\n",
    "        torch.save(trainable_params, path)\n",
    "    \n",
    "    def load_trainable_params(self, path):\n",
    "        trainable_params = torch.load(path)\n",
    "        for name, param in self.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                param.data = trainable_params.pop(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loader(data, batch_size, tokenizer, shuffle, max_len=40):\n",
    "    \"\"\"\n",
    "    Get a data loader for the training data.\n",
    "    \"\"\"\n",
    "    X, y = data['prompt'], data['rating']\n",
    "    y=y-1\n",
    "    # print(X)\n",
    "    # print(\"type of X: \", type(X))\n",
    "    # print(\"type of y: \", type(y))\n",
    "    X = tokenizer.batch_encode_plus(\n",
    "        X.tolist(), max_length=max_len, truncation=True, padding='max_length')\n",
    "    X, mask = X['input_ids'], X['attention_mask']\n",
    "    # convert them to tensors\n",
    "    X = torch.tensor(X)\n",
    "    mask = torch.tensor(mask)\n",
    "    y = torch.tensor(y.values, dtype=int)\n",
    "    data = torch.utils.data.TensorDataset(X, mask, y)\n",
    "    \n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        data, batch_size=batch_size, shuffle=shuffle)\n",
    "    \n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def train(model, train_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_pred = 0\n",
    "    total_pred = 0\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        X, mask, y = batch\n",
    "        X, mask, y = X.to(device), mask.to(device), y.to(device)\n",
    "        output = model(X, mask) # return logits for last valid token\n",
    "        \n",
    "        loss = criterion(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        correct_pred += (predicted == y).sum().item()\n",
    "        total_pred += len(y)\n",
    "    \n",
    "    return total_loss / len(train_loader), correct_pred / total_pred \n",
    "\n",
    "# def train(model, train_loader, optimizer, criterion, device):\n",
    "#     model.train()\n",
    "#     total_loss = 0\n",
    "#     total_rmse = 0\n",
    "#     for batch in train_loader:\n",
    "#         optimizer.zero_grad()\n",
    "#         X, mask, y = batch\n",
    "#         X, mask, y = X.to(device), mask.to(device), y.float().to(device)\n",
    "\n",
    "#         output = model(X, mask)\n",
    "#         loss = criterion(output, y)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         total_loss += loss.item()\n",
    "#         total_rmse += ((output - y) ** 2).sum().item()\n",
    "\n",
    "#     rmse = (total_rmse / len(train_loader.dataset)) ** 0.5\n",
    "#     return total_loss / len(train_loader), rmse\n",
    "\n",
    "\n",
    "# def evaluate(model, val_loader, criterion, device):\n",
    "#     model.eval()\n",
    "#     total_loss = 0\n",
    "#     total_rmse = 0\n",
    "#     with torch.no_grad():\n",
    "#         for batch in val_loader:\n",
    "#             X, mask, y = batch\n",
    "#             X, mask, y = X.to(device), mask.to(device), y.float().to(device)\n",
    "\n",
    "#             output = model(X, mask)\n",
    "#             loss = criterion(output, y)\n",
    "#             total_loss += loss.item()\n",
    "#             total_rmse += ((output - y) ** 2).sum().item()\n",
    "\n",
    "#     rmse = (total_rmse / len(val_loader.dataset)) ** 0.5\n",
    "#     return total_loss / len(val_loader), rmse\n",
    "\n",
    "\n",
    "def evaluate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct_pred = 0\n",
    "    total_pred = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            X, mask, y = batch\n",
    "            X, mask, y = X.to(device), mask.to(device), y.to(device)\n",
    "            output = model(X, mask)\n",
    "            loss = criterion(output, y)\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            correct_pred += (predicted == y).sum().item()\n",
    "            total_pred += len(y)\n",
    "             \n",
    "    return total_loss / len(val_loader) , correct_pred / total_pred \n",
    "    \n",
    "def plot_losses(train_losses, val_losses, mode, args):\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(val_losses, label='Val Loss')\n",
    "    plt.title(f'{mode}_{args.gpt_variant} Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'../plots_food/{args.gpt_variant}_loss.png')\n",
    "    plt.close()\n",
    "    print(f\"Plots saved at plots_food/{args.gpt_variant}_loss.png\")\n",
    "    \n",
    "def plot_metrics(train_accs, val_accs, mode, args):\n",
    "    plt.plot(train_accs, label='Train Acc')\n",
    "    plt.plot(val_accs, label='Val Acc')\n",
    "    plt.title(f'{mode} Accuracy')\n",
    "    plt.xlabel('Epoch') \n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'../plots_food/{args.gpt_variant}_acc.png')\n",
    "    plt.close()\n",
    "    print(f\"Plots saved at plots_food/{args.gpt_variant}_acc.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of train_loader:  3303\n",
      "length of val_loader:  1121\n"
     ]
    }
   ],
   "source": [
    "train_loader = get_data_loader(train_df, 256, tokenizer,True)\n",
    "val_loader = get_data_loader(val_df, 256, tokenizer,False)\n",
    "\n",
    "print(\"length of train_loader: \", len(train_loader))\n",
    "print(\"length of val_loader: \", len(val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forcing vocab_size=50257, block_size=1024, bias=True\n",
      "loading weights from pretrained gpt2 model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 124.44M\n",
      "Number of trainable parameters: 85.06M\n",
      "Reduction: 31.65%\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 1/10: Train Loss: 0.8031, Train Acc: 0.7415, Val Loss: 1.1904, Val Acc: 0.6599\n",
      "Time taken for epoch 1: 3582.9179 seconds (Training: 3228.1826 seconds, Evaluation: 354.7353 seconds)\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 2/10: Train Loss: 0.7710, Train Acc: 0.7432, Val Loss: 1.1377, Val Acc: 0.6601\n",
      "Time taken for epoch 2: 3584.9494 seconds (Training: 3229.9319 seconds, Evaluation: 355.0175 seconds)\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 3/10: Train Loss: 0.7726, Train Acc: 0.7429, Val Loss: 1.1626, Val Acc: 0.6598\n",
      "Time taken for epoch 3: 3591.4406 seconds (Training: 3236.4412 seconds, Evaluation: 354.9993 seconds)\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 4/10: Train Loss: 0.7639, Train Acc: 0.7438, Val Loss: 1.2102, Val Acc: 0.6598\n",
      "Time taken for epoch 4: 3584.0674 seconds (Training: 3227.7663 seconds, Evaluation: 356.3011 seconds)\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 5/10: Train Loss: 0.7609, Train Acc: 0.7439, Val Loss: 1.1937, Val Acc: 0.6557\n",
      "Time taken for epoch 5: 3583.6223 seconds (Training: 3228.4670 seconds, Evaluation: 355.1553 seconds)\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 6/10: Train Loss: 0.7626, Train Acc: 0.7438, Val Loss: 1.1660, Val Acc: 0.6587\n",
      "Time taken for epoch 6: 3583.7361 seconds (Training: 3229.1739 seconds, Evaluation: 354.5621 seconds)\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 7/10: Train Loss: 0.7558, Train Acc: 0.7444, Val Loss: 1.1699, Val Acc: 0.6572\n",
      "Time taken for epoch 7: 3582.3876 seconds (Training: 3227.1316 seconds, Evaluation: 355.2560 seconds)\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 8/10: Train Loss: 0.7543, Train Acc: 0.7445, Val Loss: 1.1905, Val Acc: 0.6580\n",
      "Time taken for epoch 8: 3578.5466 seconds (Training: 3223.9033 seconds, Evaluation: 354.6433 seconds)\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 9/10: Train Loss: 0.7519, Train Acc: 0.7445, Val Loss: 1.1817, Val Acc: 0.6592\n",
      "Time taken for epoch 9: 3581.1460 seconds (Training: 3226.4521 seconds, Evaluation: 354.6939 seconds)\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 10/10: Train Loss: 0.7509, Train Acc: 0.7449, Val Loss: 1.1945, Val Acc: 0.6568\n",
      "Time taken for epoch 10: 2206.5285 seconds (Training: 2049.2210 seconds, Evaluation: 157.3075 seconds)\n",
      "Metrics saved at model/gpt2_metrics.txt\n",
      "Plots saved at plots_food/gpt2_loss.png\n",
      "Plots saved at plots_food/gpt2_acc.png\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "# Define the main function and other necessary functions\n",
    "def main(args):\n",
    "    if args.mode == \"gen\":\n",
    "        model = GPT(args.gpt_variant, is_gen=True).to(args.device)\n",
    "        model.eval()\n",
    "\n",
    "        # TODO: You can add your super creative prompt here\n",
    "        prompt = \"My name is Inigo Montoya. You killed my father. Prepare to die. \"\n",
    "        # prompt = \"Once upon a time, in a land far far away, there was a\"\n",
    "\n",
    "        input_ids = tokenizer.encode(prompt, return_tensors='pt').to(args.device)\n",
    "        output = model.generate(input_ids, max_new_tokens=args.max_new_tokens)\n",
    "        print(\"\", tokenizer.decode(output[0]), sep=\"\\n\")\n",
    "\n",
    "    elif args.mode == \"GPT2model\":    \n",
    "        model = GPT(args.gpt_variant).to(args.device)\n",
    "        \n",
    "        # TODO: Implement the training loop (fill the train and evaluate functions in train_utils.py)\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        train_accs = []\n",
    "        val_accs = []\n",
    "        \n",
    "        for epoch in range(args.epochs):\n",
    "            print('-' * 80)\n",
    "            epoch_start = time.time()\n",
    "\n",
    "            train_start= time.time()\n",
    "            train_loss,train_acc = train(model, train_loader, optimizer, criterion, args.device)\n",
    "            train_end= time.time()\n",
    "\n",
    "            val_start= time.time()\n",
    "            val_loss,val_acc = evaluate(model, val_loader, criterion, args.device)\n",
    "            val_end= time.time()\n",
    "\n",
    "            epoch_end = time.time()\n",
    "\n",
    "            # Calculate the durations\n",
    "            train_duration = train_end - train_start\n",
    "            val_duration = val_end - val_start\n",
    "            epoch_duration = epoch_end - epoch_start\n",
    "    \n",
    "            train_losses.append(train_loss)\n",
    "            val_losses.append(val_loss)\n",
    "            train_accs.append(train_acc)\n",
    "            val_accs.append(val_acc)\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}/{args.epochs}: Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "            print(f\"Time taken for epoch {epoch+1}: {epoch_duration:.4f} seconds (Training: {train_duration:.4f} seconds, Evaluation: {val_duration:.4f} seconds)\")\n",
    "            # save the model after every 10 epochs in model directory with different names use save_trainable_params\n",
    "            # if (epoch + 1) % 25 == 0:\n",
    "            #     model.save_trainable_params(f'model/{args.gpt_variant}_{epoch+1}.pth')\n",
    "\n",
    "        with open(f'../results_food/{args.gpt_variant}_metrics.txt', 'w') as f:\n",
    "            f.write('Train Losses:\\n')\n",
    "            f.write(str(train_losses))\n",
    "            f.write('\\nVal Losses:\\n')\n",
    "            f.write(str(val_losses))\n",
    "            f.write('\\nTrain Accuracies:\\n')\n",
    "            f.write(str(train_accs))\n",
    "            f.write('\\nVal Accuracies:\\n')\n",
    "        print(f\"Metrics saved at model/{args.gpt_variant}_metrics.txt\") \n",
    "        \n",
    "        # TODO: Also plot the training losses and metrics\n",
    "        # save the plot in plots folder \n",
    "        plot_losses(train_losses, val_losses, args.mode, args)\n",
    "        plot_metrics(train_accs, val_accs, args.mode, args)\n",
    "        # model.save_trainable_params(args.model_path)\n",
    "    else:\n",
    "        print(\"Invalid mode\")\n",
    "        return\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    class Args:\n",
    "        mode = \"GPT2model\"\n",
    "        gpu_id = 2\n",
    "        gpt_variant = \"gpt2\"\n",
    "        max_new_tokens = 100\n",
    "        model_path = \"models/GPT2model.pth\"\n",
    "        lr = 1e-3\n",
    "        batch_size = 64\n",
    "        epochs = 10\n",
    "        device = torch.device(\n",
    "            \"cuda:2\" if torch.cuda.is_available() and gpu_id >= 0 else\\\n",
    "            \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "        )\n",
    "\n",
    "    args = Args()\n",
    "    main(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
