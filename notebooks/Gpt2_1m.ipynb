{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c3251f5",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "class SampleGenerator(object):\n",
    "    \"\"\"Construct dataset for multi-class classification\"\"\"\n",
    "\n",
    "    def __init__(self, ratings, n_test=1):\n",
    "        \"\"\"\n",
    "        args:\n",
    "            ratings: pd.DataFrame, which contains 4 columns = ['userId', 'itemId', 'rating', 'timestamp']\n",
    "        \"\"\"\n",
    "        assert 'userId' in ratings.columns\n",
    "        assert 'itemId' in ratings.columns\n",
    "        assert 'rating' in ratings.columns\n",
    "\n",
    "        self.ratings = ratings\n",
    "        # self.preprocessed_ratings = self._preprocess_ratings(ratings)\n",
    "        self.train_ratings, self.test_ratings = self._split_loo(self.ratings, n_test)\n",
    "\n",
    "    def _split_loo(self, ratings, n_test=1):\n",
    "        \"\"\"\n",
    "        Split dataset into train and test sets, with `n_test` interactions per user in the test set.\n",
    "\n",
    "        Args:\n",
    "            ratings: DataFrame, contains user-item interactions.\n",
    "            n_test: int, number of recent interactions to include in the test set.\n",
    "\n",
    "        Returns:\n",
    "            train: DataFrame, contains the train set.\n",
    "            test: DataFrame, contains the test set.\n",
    "        \"\"\"\n",
    "        # Rank interactions by timestamp for each user\n",
    "        ratings['rank_latest'] = ratings.groupby(['userId'])['timestamp'].rank(method='first', ascending=False)\n",
    "\n",
    "        # Test set contains the top `n_test` interactions for each user\n",
    "        test = ratings[ratings['rank_latest'] <= n_test]\n",
    "        \n",
    "        # Train set contains all other interactions\n",
    "        train = ratings[ratings['rank_latest'] > n_test]\n",
    "\n",
    "        # Ensure every user is in both train and test sets\n",
    "        assert train['userId'].nunique() == test['userId'].nunique()\n",
    "\n",
    "        # Return train and test dataframes\n",
    "        return train[['userId', 'itemId', 'rating']], test[['userId', 'itemId', 'rating']]\n",
    "\n",
    "    def evaluate_data(self):\n",
    "        \"\"\"Create evaluate data for classification\"\"\"\n",
    "        test_users = self.test_ratings['userId'].tolist()\n",
    "        test_items = self.test_ratings['itemId'].tolist()\n",
    "        test_ratings = self.test_ratings['rating'].tolist()\n",
    "\n",
    "        return [torch.LongTensor(test_users), torch.LongTensor(test_items), torch.LongTensor(test_ratings)]\n",
    "    \n",
    "    def get_train_test_dataframes(self):\n",
    "        \"\"\"\n",
    "        Returns the train and test dataframes.\n",
    "        \"\"\"\n",
    "        return self.train_ratings, self.test_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88d0679a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Range of userId is [0, 6039]\n",
      "Range of itemId is [0, 3705]\n",
      "(909609, 3) (90600, 3)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "\n",
    "# Load Data\n",
    "data_dir = '/raid/home/shyamsg/Final_Project/DecoderOnlyModel/Data/u1m.dat'\n",
    "rating_df = pd.read_csv(data_dir, sep='::', header=None, names=['uid', 'mid', 'rating', 'timestamp'], engine='python')\n",
    "\n",
    "# i want to drop nulls\n",
    "rating_df = rating_df.dropna()\n",
    "\n",
    "# Reindex userId and itemId, start from 0\n",
    "user_id = rating_df[['uid']].drop_duplicates().reindex()\n",
    "user_id['userId'] = np.arange(len(user_id))\n",
    "rating_df = pd.merge(rating_df, user_id, on=['uid'], how='left')\n",
    "\n",
    "# Reindex itemId, start from 0\n",
    "item_id = rating_df[['mid']].drop_duplicates()\n",
    "item_id['itemId'] = np.arange(len(item_id))\n",
    "rating_df = pd.merge(rating_df, item_id, on=['mid'], how='left')\n",
    "\n",
    "rating_df = rating_df[['userId', 'itemId', 'rating', 'timestamp']]\n",
    "\n",
    "print('Range of userId is [{}, {}]'.format(rating_df.userId.min(), rating_df.userId.max()))\n",
    "print('Range of itemId is [{}, {}]'.format(rating_df.itemId.min(), rating_df.itemId.max()))\n",
    "\n",
    "\n",
    "# n_test specifies number of most recent interactions to be used for testing for each user\n",
    "sample_generator = SampleGenerator(rating_df, n_test=15)\n",
    "\n",
    "# Get train and test dataframes\n",
    "df1, df2 = sample_generator.get_train_test_dataframes()\n",
    "\n",
    "print(df1.shape, df2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18b1a48c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(909609, 2)\n",
      "(90600, 2)\n",
      "Train Size: 909609, Val Size: 90600\n"
     ]
    }
   ],
   "source": [
    "# now create a new dataframe columns\n",
    "#df['prompt'] = What star rating do you think userid_df[user_id] will give item_df[item_id]? \n",
    "#df['rating'] = df[rating]\n",
    "\n",
    "train_df = pd.DataFrame()\n",
    "train_df['prompt'] = \"What star rating do you think user_\" + df1['userId'].astype(str) + \" will give item_\" + df1['itemId'].astype(str) + \"?\"\n",
    "train_df['rating'] = df1['rating']\n",
    "print(train_df.shape)   \n",
    "\n",
    "val_df = pd.DataFrame()\n",
    "val_df['prompt'] = \"What star rating do you think user_\" + df2['userId'].astype(str) + \" will give item_\" + df2['itemId'].astype(str) + \"?\"\n",
    "val_df['rating'] = df2['rating']\n",
    "print(val_df.shape)\n",
    "\n",
    "print(f\"Train Size: {len(train_df)}, Val Size: {len(val_df)}\")\n",
    "          \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b8c0966",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid/home/shyamsg/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Full definition of a GPT Language Model, all of it in this single file.\n",
    "References:\n",
    "1) the official GPT-2 TensorFlow implementation released by OpenAI:\n",
    "https://github.com/openai/gpt-2/blob/master/src/model.py\n",
    "2) huggingface/transformers PyTorch implementation:\n",
    "https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py\n",
    "Code adopted from: https://github.com/karpathy/nanoGPT\n",
    "\"\"\"\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from dataclasses import dataclass\n",
    "from transformers import GPT2LMHeadModel, GPT2ForSequenceClassification\n",
    "\n",
    "\n",
    "################################################################################\n",
    "# GPT Model classes\n",
    "################################################################################\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024\n",
    "    vocab_size: int = 50304\n",
    "    n_layer: int = 12\n",
    "    n_head: int = 12\n",
    "    n_embd: int = 768\n",
    "    dropout: float = 0.0\n",
    "    bias: bool = True\n",
    "\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    \"\"\" LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False \"\"\"\n",
    "    def __init__(self, ndim, bias):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(ndim))\n",
    "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
    "\n",
    "    def forward(self, input):\n",
    "        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)\n",
    "\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        # regularization\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.dropout = config.dropout\n",
    "        # flash attention, but support is only in PyTorch >= 2.0\n",
    "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
    "        if not self.flash:\n",
    "            # causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "            self.register_buffer(\"bias\", torch.tril(\n",
    "                torch.ones(config.block_size, config.block_size))\n",
    "                .view(1, 1, config.block_size, config.block_size))\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "\n",
    "\n",
    "        attn_out = self.c_attn(x)\n",
    "\n",
    "        # TODO: send x through the auxiliary c_attn and add them back to attn_out\n",
    "        # x = self.c_attn_GPT2model(x)\n",
    "        # attn_out = attn_out + x\n",
    "        # print(f\"shape of x in CausalSelfAttention: {attn_out.shape}\")\n",
    "\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        q, k, v  = attn_out.split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        if self.flash:\n",
    "            # efficient attention using Flash Attention CUDA kernels\n",
    "            y = torch.nn.functional.scaled_dot_product_attention(\n",
    "                q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)\n",
    "        else:\n",
    "            # manual implementation of attention\n",
    "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "            att = F.softmax(att, dim=-1)\n",
    "            att = self.attn_dropout(att)\n",
    "            y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        proj_out = self.c_proj(y)\n",
    "\n",
    "        # TODO: send y through the auxiliary c_proj and add them back to proj_out\n",
    "        # added by me\n",
    "        # proj_out = proj_out + self.c_proj_GPT2model(y)\n",
    "        # added by me \n",
    "\n",
    "        y = self.resid_dropout(proj_out)\n",
    "        \n",
    "        # print(f\"shape of y after residual dropout: {y.shape}\")\n",
    "        return y\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
    "        self.gelu    = nn.GELU()\n",
    "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "        # added by me \n",
    "\n",
    "    def forward(self, x):\n",
    "        # commented by me which they have given\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)\n",
    "    \n",
    "        return x\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, model_type='gpt2', is_gen=False):\n",
    "        super(GPT, self).__init__()\n",
    "        \n",
    "        # n_layer, n_head and n_embd are determined from model_type\n",
    "        config_args = {\n",
    "            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n",
    "            'gpt2-small':  dict(n_layer=12, n_head=12, n_embd=768), # later last 6 layers will be removed\n",
    "            # 'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n",
    "            # 'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n",
    "            # 'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M (1.3B) params\n",
    "        }[model_type]\n",
    "        print(\"forcing vocab_size=50257, block_size=1024, bias=True\")\n",
    "        config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints\n",
    "        config_args['block_size'] = 1024  # always 1024 for GPT model checkpoints\n",
    "        config_args['bias'] = True        # always True for GPT model checkpoints\n",
    "\n",
    "        self.config = GPTConfig(**config_args)\n",
    "        \n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(self.config.vocab_size, self.config.n_embd),\n",
    "            wpe = nn.Embedding(self.config.block_size, self.config.n_embd),\n",
    "            drop = nn.Dropout(self.config.dropout),\n",
    "            h = nn.ModuleList([Block(self.config) for _ in range(self.config.n_layer)]),\n",
    "            ln_f = LayerNorm(self.config.n_embd, bias=self.config.bias),\n",
    "        ))\n",
    "        if is_gen:\n",
    "            self.lm_head = nn.Linear(\n",
    "                self.config.n_embd, self.config.vocab_size, bias=False)\n",
    "            self.transformer.wte.weight = self.lm_head.weight  # https://paperswithcode.com/method/weight-tying\n",
    "            self.score = None\n",
    "        else:\n",
    "            self.score = nn.Linear(self.config.n_embd, 5, bias=False)\n",
    "\n",
    "        # TODO: Remove gradients from the embedding layers\n",
    "        # added by me below\n",
    "        self.transformer.wte.weight.requires_grad = False\n",
    "        self.transformer.wpe.weight.requires_grad = False\n",
    "        # added by me above \n",
    "\n",
    "        sd = self.state_dict()\n",
    "        sd_keys = sd.keys()\n",
    "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')]\n",
    "\n",
    "        # init a huggingface/transformers model\n",
    "        print(\"loading weights from pretrained gpt2 model\")\n",
    "        if is_gen:\n",
    "            model_hf = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "        else:\n",
    "            model_hf = GPT2ForSequenceClassification.from_pretrained('gpt2')\n",
    "\n",
    "        sd_hf = model_hf.state_dict()\n",
    "\n",
    "        # copy while ensuring all of the parameters are aligned and match in names and shapes\n",
    "        sd_keys_hf = sd_hf.keys()\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')]\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')]\n",
    "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
    "        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
    "        # intialize the score.weight with xavier initialization \n",
    "        for k in sd_keys:\n",
    "            if k.endswith('score.weight'):\n",
    "                nn.init.xavier_uniform_(sd[k])\n",
    "        # remove the score.weigth from the list of keys\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('score.weight')]\n",
    "        for k in sd_keys_hf:\n",
    "            if any(k.endswith(w) for w in transposed):\n",
    "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k].t())\n",
    "            else:\n",
    "                # vanilla copy over the other parameters except the params that are not in the pretrained model\n",
    "                assert sd_hf[k].shape == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k])\n",
    "                    \n",
    "        # Remove the last 6 layers for gpt2-small variant\n",
    "        if model_type == 'gpt2-small':\n",
    "            self.transformer.h = self.transformer.h[:6]\n",
    "                    \n",
    "        # print the total number of parameters\n",
    "        total_params = sum(p.numel() for p in self.parameters())\n",
    "        print(f\"Number of parameters: {total_params / 1e6:.2f}M\")\n",
    "        if not is_gen:\n",
    "            # print the number of trainable parameters\n",
    "            num_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "            print(f\"Number of trainable parameters: {num_params / 1e6:.2f}M\")\n",
    "            # calculate the reduction in parameters\n",
    "            reduction = 100 * (total_params - num_params) / total_params\n",
    "            print(f\"Reduction: {reduction:.2f}%\")\n",
    "\n",
    "    def forward(self, idx, mask=None):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device) # shape (t)\n",
    "\n",
    "        # if mask is provided, find the indices of the last tokens in each sequence\n",
    "        if mask is not None:\n",
    "            assert mask.size() == idx.size(), \"Mask size must match input size\"\n",
    "            eos_idxs = mask.sum(1) - 1 # last non-pad token in each sequence\n",
    "\n",
    "        # forward the GPT model itself\n",
    "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
    "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (t, n_embd)\n",
    "        x = self.transformer.drop(tok_emb + pos_emb)\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "\n",
    "        if self.score is None:\n",
    "            # inference-time mini-optimization: only forward the lm_head on the very last position\n",
    "            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n",
    "        else: # no need to preserve the time dimension for classification task\n",
    "            if mask is not None:\n",
    "                # if mask is provided, only return the logits for the last token in each sequence\n",
    "                logits = self.score(x[torch.arange(b, device=device), eos_idxs])\n",
    "            else:\n",
    "                \n",
    "                logits = self.score(x[:, -1, :])\n",
    "               \n",
    "        return logits\n",
    "\n",
    "    def crop_block_size(self, block_size):\n",
    "        # model surgery to decrease the block size if necessary\n",
    "        # e.g. we may load the GPT2 pretrained model checkpoint (block size 1024)\n",
    "        # but want to use a smaller block size for some smaller, simpler model\n",
    "        assert block_size <= self.config.block_size\n",
    "        self.config.block_size = block_size\n",
    "        self.transformer.wpe.weight = nn.Parameter(self.transformer.wpe.weight[:block_size])\n",
    "        for block in self.transformer.h:\n",
    "            if hasattr(block.attn, 'bias'):\n",
    "                block.attn.bias = block.attn.bias[:,:,:block_size,:block_size]\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        \"\"\"\n",
    "        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
    "        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
    "        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
    "        \"\"\"\n",
    "        \n",
    "       \n",
    "        for _ in range(max_new_tokens):\n",
    "            # if the sequence context is growing too long we must crop it at block_size\n",
    "            \n",
    "            # print(f\"shape of idx in generate: {idx.shape}\") # added by me \n",
    "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
    "            # forward the model to get the logits for the index in the sequence\n",
    "            logits = self(idx_cond)\n",
    "            # pluck the logits at the final step and scale by desired temperature\n",
    "            \n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            # optionally crop the logits to only the top k options\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "            # apply softmax to convert logits to (normalized) probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            # append sampled index to the running sequence and continue\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n",
    "    \n",
    "    def save_trainable_params(self, path):\n",
    "        trainable_params =\\\n",
    "            list(filter(lambda p: p.requires_grad, self.parameters()))\n",
    "        torch.save(trainable_params, path)\n",
    "    \n",
    "    def load_trainable_params(self, path):\n",
    "        trainable_params = torch.load(path)\n",
    "        for name, param in self.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                param.data = trainable_params.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "281eb4ed",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def get_data_loader(data, batch_size, tokenizer, shuffle, max_len=40):\n",
    "    \"\"\"\n",
    "    Get a data loader for the training data.\n",
    "    \"\"\"\n",
    "    X, y = data['prompt'], data['rating']\n",
    "    y= y-1\n",
    "    # print(X)\n",
    "    # print(\"type of X: \", type(X))\n",
    "    # print(\"type of y: \", type(y))\n",
    "    X = tokenizer.batch_encode_plus(\n",
    "        X.tolist(), max_length=max_len, truncation=True, padding='max_length')\n",
    "    X, mask = X['input_ids'], X['attention_mask']\n",
    "    # convert them to tensors\n",
    "    X = torch.tensor(X)\n",
    "    mask = torch.tensor(mask)\n",
    "    y = torch.tensor(y.values, dtype=int)\n",
    "    data = torch.utils.data.TensorDataset(X, mask, y)\n",
    "    \n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        data, batch_size=batch_size, shuffle=shuffle)\n",
    "    \n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "022a5acd",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33f46b18",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def train(model, train_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_pred = 0\n",
    "    total_pred = 0\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        X, mask, y = batch\n",
    "        X, mask, y = X.to(device), mask.to(device), y.to(device)\n",
    "        output = model(X, mask) # return logits for last valid token\n",
    "        \n",
    "        loss = criterion(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        correct_pred += (predicted == y).sum().item()\n",
    "        total_pred += len(y)\n",
    "    \n",
    "    return total_loss / len(train_loader), correct_pred / total_pred \n",
    "  \n",
    "def evaluate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct_pred = 0\n",
    "    total_pred = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            X, mask, y = batch\n",
    "            X, mask, y = X.to(device), mask.to(device), y.to(device)\n",
    "            output = model(X, mask)\n",
    "            loss = criterion(output, y)\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            correct_pred += (predicted == y).sum().item()\n",
    "            total_pred += len(y)\n",
    "             \n",
    "    return total_loss / len(val_loader) , correct_pred / total_pred \n",
    "\n",
    "    \n",
    "def plot_losses(train_losses, val_losses, mode, args):\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(val_losses, label='Val Loss')\n",
    "    plt.title(f'{mode}_{args.gpt_variant} Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'plots_1m/{args.gpt_variant}_loss.png')\n",
    "    plt.close()\n",
    "    print(f\"Plots saved at plots_1m/{args.gpt_variant}_loss.png\")\n",
    "    \n",
    "def plot_metrics(train_accs, val_accs, mode, args):\n",
    "    plt.plot(train_accs, label='Train Acc')\n",
    "    plt.plot(val_accs, label='Val Acc')\n",
    "    plt.title(f'{mode} Accuracy')\n",
    "    plt.xlabel('Epoch') \n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'plots_1m/{args.gpt_variant}_acc.png')\n",
    "    plt.close()\n",
    "    print(f\"Plots saved at plots_1m/{args.gpt_variant}_acc.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd92f027",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train_loader:  1777\n",
      "Length of val_loader:  177\n"
     ]
    }
   ],
   "source": [
    "train_loader = get_data_loader(train_df, 512, tokenizer,True)\n",
    "val_loader = get_data_loader(val_df, 512, tokenizer,False)\n",
    "\n",
    "print(\"Length of train_loader: \", len(train_loader))\n",
    "print(\"Length of val_loader: \", len(val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64d71ff8",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forcing vocab_size=50257, block_size=1024, bias=True\n",
      "loading weights from pretrained gpt2 model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 124.44M\n",
      "Number of trainable parameters: 85.06M\n",
      "Reduction: 31.65%\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 1/10: Train Loss: 1.4395, Train Acc: 0.3633, Val Loss: 1.4046, Val Acc: 0.3651\n",
      "Time taken for epoch 1: 2710.7518 seconds (Training: 2600.5023 seconds, Evaluation: 110.2495 seconds)\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 2/10: Train Loss: 1.3285, Train Acc: 0.4025, Val Loss: 1.3579, Val Acc: 0.3967\n",
      "Time taken for epoch 2: 3473.3021 seconds (Training: 3363.1650 seconds, Evaluation: 110.1371 seconds)\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 3/10: Train Loss: 1.2701, Train Acc: 0.4315, Val Loss: 1.3171, Val Acc: 0.4136\n",
      "Time taken for epoch 3: 3472.4313 seconds (Training: 3362.6693 seconds, Evaluation: 109.7620 seconds)\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 4/10: Train Loss: 1.2420, Train Acc: 0.4452, Val Loss: 1.3037, Val Acc: 0.4192\n",
      "Time taken for epoch 4: 3483.2553 seconds (Training: 3373.4240 seconds, Evaluation: 109.8312 seconds)\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 5/10: Train Loss: 1.2272, Train Acc: 0.4516, Val Loss: 1.2877, Val Acc: 0.4265\n",
      "Time taken for epoch 5: 3473.7006 seconds (Training: 3363.5305 seconds, Evaluation: 110.1701 seconds)\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 6/10: Train Loss: 1.2142, Train Acc: 0.4573, Val Loss: 1.2882, Val Acc: 0.4258\n",
      "Time taken for epoch 6: 3473.2309 seconds (Training: 3363.0982 seconds, Evaluation: 110.1327 seconds)\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 7/10: Train Loss: 1.2038, Train Acc: 0.4624, Val Loss: 1.2717, Val Acc: 0.4327\n",
      "Time taken for epoch 7: 3471.4536 seconds (Training: 3361.2859 seconds, Evaluation: 110.1678 seconds)\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 8/10: Train Loss: 1.1940, Train Acc: 0.4663, Val Loss: 1.2692, Val Acc: 0.4361\n",
      "Time taken for epoch 8: 3473.9720 seconds (Training: 3363.4280 seconds, Evaluation: 110.5441 seconds)\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 9/10: Train Loss: 1.1857, Train Acc: 0.4701, Val Loss: 1.2601, Val Acc: 0.4369\n",
      "Time taken for epoch 9: 3476.1844 seconds (Training: 3366.1960 seconds, Evaluation: 109.9885 seconds)\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 10/10: Train Loss: 1.1772, Train Acc: 0.4740, Val Loss: 1.2602, Val Acc: 0.4376\n",
      "Time taken for epoch 10: 3479.5741 seconds (Training: 3369.0611 seconds, Evaluation: 110.5130 seconds)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'results/gpt2_metrics.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 100\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid mode\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     99\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 100\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 82\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTime taken for epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch_duration\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds (Training: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_duration\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds, Evaluation: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_duration\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;66;03m# save the model after every 10 epochs in model directory with different names use save_trainable_params\u001b[39;00m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;66;03m# if (epoch + 1) % 25 == 0:\u001b[39;00m\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;66;03m#     model.save_trainable_params(f'models/{args.gpt_variant}_{epoch+1}.pth')\u001b[39;00m\n\u001b[0;32m---> 82\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mresults/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgpt_variant\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_metrics.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     83\u001b[0m     f\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrain Losses:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     84\u001b[0m     f\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;28mstr\u001b[39m(train_losses))\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m     )\n\u001b[0;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'results/gpt2_metrics.txt'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Define the arguments in a dictionary\n",
    "args = {\n",
    "    \"mode\": \"GPT2model\",  # Change this to \"GPT2model\"\n",
    "    \"gpu_id\": 4,\n",
    "    \"gpt_variant\": \"gpt2\", # Change this to \"gpt2\",\"gpt2-small\"\n",
    "    \"max_new_tokens\": 100,\n",
    "    \"model_path\": \"models/GPT2model.pth\",\n",
    "    \"lr\": 1e-3,\n",
    "    # \"batch_size\": 64,\n",
    "    \"epochs\": 10\n",
    "}\n",
    "\n",
    "# Convert the dictionary to an object with attributes\n",
    "class Args:\n",
    "    def __init__(self, **entries):\n",
    "        self.__dict__.update(entries)\n",
    "\n",
    "args = Args(**args)\n",
    "args.device = torch.device(\n",
    "\"cuda:2\" if torch.cuda.is_available() and args.gpu_id >= 0 else\\\n",
    "    \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "# args.device = torch.device(\"cpu\")\n",
    "\n",
    "# Define the main function and other necessary functions\n",
    "def main(args):\n",
    "    if args.mode == \"gen\":\n",
    "        model = GPT(args.gpt_variant, is_gen=True).to(args.device)\n",
    "        model.eval()\n",
    "\n",
    "        # Creative prompt for text generation\n",
    "        prompt = \"Once upon a time, in a land far far away, there was a\"\n",
    "        input_ids = tokenizer.encode(prompt, return_tensors='pt').to(args.device)\n",
    "        output = model.generate(input_ids, max_new_tokens=args.max_new_tokens)\n",
    "        print(\"\", tokenizer.decode(output[0]), sep=\"\\n\")\n",
    "\n",
    "    elif args.mode == \"GPT2model\":    \n",
    "        model = GPT(args.gpt_variant).to(args.device)\n",
    "        \n",
    "        # TODO: Implement the training loop (fill the train and evaluate functions in train_utils.py)\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        train_accs = []\n",
    "        val_accs = []\n",
    "        \n",
    "        for epoch in range(args.epochs):\n",
    "            print('-' * 80)\n",
    "            epoch_start = time.time()\n",
    "\n",
    "            train_start= time.time()\n",
    "            train_loss,train_acc = train(model, train_loader, optimizer, criterion, args.device)\n",
    "            train_end= time.time()\n",
    "\n",
    "            val_start= time.time()\n",
    "            val_loss,val_acc = evaluate(model, val_loader, criterion, args.device)\n",
    "            val_end= time.time()\n",
    "\n",
    "            epoch_end = time.time()\n",
    "\n",
    "            # Calculate the durations\n",
    "            train_duration = train_end - train_start\n",
    "            val_duration = val_end - val_start\n",
    "            epoch_duration = epoch_end - epoch_start\n",
    "    \n",
    "            train_losses.append(train_loss)\n",
    "            val_losses.append(val_loss)\n",
    "            train_accs.append(train_acc)\n",
    "            val_accs.append(val_acc)\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}/{args.epochs}: Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "            print(f\"Time taken for epoch {epoch+1}: {epoch_duration:.4f} seconds (Training: {train_duration:.4f} seconds, Evaluation: {val_duration:.4f} seconds)\")\n",
    "            # save the model after every 10 epochs in model directory with different names use save_trainable_params\n",
    "            # if (epoch + 1) % 25 == 0:\n",
    "            #     model.save_trainable_params(f'models/{args.gpt_variant}_{epoch+1}.pth')\n",
    "\n",
    "        with open(f'results/{args.gpt_variant}_metrics.txt', 'w') as f:\n",
    "            f.write('Train Losses:\\n')\n",
    "            f.write(str(train_losses))\n",
    "            f.write('\\nVal Losses:\\n')\n",
    "            f.write(str(val_losses))\n",
    "            f.write('\\nTrain Accuracies:\\n')\n",
    "            f.write(str(train_accs))\n",
    "            f.write('\\nVal Accuracies:\\n')\n",
    "        print(f\"Metrics saved at model/{args.gpt_variant}_metrics.txt\") \n",
    "        \n",
    "        # # TODO: Also plot the training losses and metrics\n",
    "        # # save the plot in plots_1m folder \n",
    "        plot_losses(train_losses, val_losses, args.mode, args)\n",
    "        plot_metrics(train_accs, val_accs, args.mode, args)\n",
    "        # model.save_trainable_params(args.model_path)\n",
    "    else:\n",
    "        print(\"Invalid mode\")\n",
    "        return\n",
    "main(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4443a665",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# coding: utf-8",
   "executable": "/usr/bin/env python",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
