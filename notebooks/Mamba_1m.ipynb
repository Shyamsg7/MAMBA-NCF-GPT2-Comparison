{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from mamba_ssm import Mamba\n",
    "import torch.nn.init as init\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid/home/shyamsg/.local/lib/python3.8/site-packages/mamba_ssm/ops/selective_scan_interface.py:164: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  def forward(ctx, xz, conv1d_weight, conv1d_bias, x_proj_weight, delta_proj_weight,\n",
      "/raid/home/shyamsg/.local/lib/python3.8/site-packages/mamba_ssm/ops/selective_scan_interface.py:240: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(ctx, dout):\n",
      "/raid/home/shyamsg/.local/lib/python3.8/site-packages/mamba_ssm/ops/triton/layer_norm.py:986: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  def forward(\n",
      "/raid/home/shyamsg/.local/lib/python3.8/site-packages/mamba_ssm/ops/triton/layer_norm.py:1045: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(ctx, dout, *args):\n",
      "/raid/home/shyamsg/.local/lib/python3.8/site-packages/mamba_ssm/distributed/tensor_parallel.py:26: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  def forward(ctx, x, weight, bias, process_group=None, sequence_parallel=True):\n",
      "/raid/home/shyamsg/.local/lib/python3.8/site-packages/mamba_ssm/distributed/tensor_parallel.py:62: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(ctx, grad_output):\n",
      "/raid/home/shyamsg/.local/lib/python3.8/site-packages/mamba_ssm/ops/triton/ssd_combined.py:758: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  def forward(ctx, zxbcdt, conv1d_weight, conv1d_bias, dt_bias, A, D, chunk_size, initial_states=None, seq_idx=None, dt_limit=(0.0, float(\"inf\")), return_final_states=False, activation=\"silu\",\n",
      "/raid/home/shyamsg/.local/lib/python3.8/site-packages/mamba_ssm/ops/triton/ssd_combined.py:836: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(ctx, dout, *args):\n",
      "/raid/home/shyamsg/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "causal_conv1d not found\n"
     ]
    }
   ],
   "source": [
    "from einops import rearrange, repeat\n",
    "\n",
    "from mamba_ssm.ops.selective_scan_interface import selective_scan_fn, mamba_inner_fn\n",
    "\n",
    "try:\n",
    "    from causal_conv1d import causal_conv1d_fn, causal_conv1d_update\n",
    "except ImportError:\n",
    "    print(\"causal_conv1d not found\")\n",
    "    causal_conv1d_fn, causal_conv1d_update = None, None\n",
    "\n",
    "try:\n",
    "    from mamba_ssm.ops.triton.selective_state_update import selective_state_update\n",
    "except ImportError:\n",
    "    print(\"selective_state_update not found\")\n",
    "    selective_state_update = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "class SampleGenerator(object):\n",
    "    \"\"\"Construct dataset for multi-class classification\"\"\"\n",
    "\n",
    "    def __init__(self, ratings, n_test=1):\n",
    "        \"\"\"\n",
    "        args:\n",
    "            ratings: pd.DataFrame, which contains 4 columns = ['userId', 'itemId', 'rating', 'timestamp']\n",
    "        \"\"\"\n",
    "        assert 'userId' in ratings.columns\n",
    "        assert 'itemId' in ratings.columns\n",
    "        assert 'rating' in ratings.columns\n",
    "\n",
    "        self.ratings = ratings\n",
    "        # self.preprocessed_ratings = self._preprocess_ratings(ratings)\n",
    "        self.train_ratings, self.test_ratings = self._split_loo(self.ratings, n_test)\n",
    "\n",
    "    def _split_loo(self, ratings, n_test=1):\n",
    "        \"\"\"\n",
    "        Split dataset into train and test sets, with `n_test` interactions per user in the test set.\n",
    "\n",
    "        Args:\n",
    "            ratings: DataFrame, contains user-item interactions.\n",
    "            n_test: int, number of recent interactions to include in the test set.\n",
    "\n",
    "        Returns:\n",
    "            train: DataFrame, contains the train set.\n",
    "            test: DataFrame, contains the test set.\n",
    "        \"\"\"\n",
    "        # Rank interactions by timestamp for each user\n",
    "        ratings['rank_latest'] = ratings.groupby(['userId'])['timestamp'].rank(method='first', ascending=False)\n",
    "\n",
    "        # Test set contains the top `n_test` interactions for each user\n",
    "        test = ratings[ratings['rank_latest'] <= n_test]\n",
    "        \n",
    "        # Train set contains all other interactions\n",
    "        train = ratings[ratings['rank_latest'] > n_test]\n",
    "\n",
    "        # Ensure every user is in both train and test sets\n",
    "        assert train['userId'].nunique() == test['userId'].nunique()\n",
    "\n",
    "        # Return train and test dataframes\n",
    "        return train[['userId', 'itemId', 'rating']], test[['userId', 'itemId', 'rating']]\n",
    "\n",
    "    def evaluate_data(self):\n",
    "        \"\"\"Create evaluate data for classification\"\"\"\n",
    "        test_users = self.test_ratings['userId'].tolist()\n",
    "        test_items = self.test_ratings['itemId'].tolist()\n",
    "        test_ratings = self.test_ratings['rating'].tolist()\n",
    "\n",
    "        return [torch.LongTensor(test_users), torch.LongTensor(test_items), torch.LongTensor(test_ratings)]\n",
    "    \n",
    "    def get_train_test_dataframes(self):\n",
    "        \"\"\"\n",
    "        Returns the train and test dataframes.\n",
    "        \"\"\"\n",
    "        return self.train_ratings, self.test_ratings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Range of userId is [0, 6039]\n",
      "Range of itemId is [0, 3705]\n",
      "(909609, 3) (90600, 3)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "\n",
    "# Load Data\n",
    "data_dir = '/raid/home/shyamsg/Final_Project/DecoderOnlyModel/Data/u1m.dat'\n",
    "rating_df = pd.read_csv(data_dir, sep='::', header=None, names=['uid', 'mid', 'rating', 'timestamp'], engine='python')\n",
    "\n",
    "# i want to drop nulls\n",
    "rating_df = rating_df.dropna()\n",
    "\n",
    "# Reindex userId and itemId, start from 0\n",
    "user_id = rating_df[['uid']].drop_duplicates().reindex()\n",
    "user_id['userId'] = np.arange(len(user_id))\n",
    "rating_df = pd.merge(rating_df, user_id, on=['uid'], how='left')\n",
    "\n",
    "# Reindex itemId, start from 0\n",
    "item_id = rating_df[['mid']].drop_duplicates()\n",
    "item_id['itemId'] = np.arange(len(item_id))\n",
    "rating_df = pd.merge(rating_df, item_id, on=['mid'], how='left')\n",
    "\n",
    "rating_df = rating_df[['userId', 'itemId', 'rating', 'timestamp']]\n",
    "\n",
    "print('Range of userId is [{}, {}]'.format(rating_df.userId.min(), rating_df.userId.max()))\n",
    "print('Range of itemId is [{}, {}]'.format(rating_df.itemId.min(), rating_df.itemId.max()))\n",
    "\n",
    "\n",
    "# n_test specifies number of most recent interactions to be used for testing for each user\n",
    "sample_generator = SampleGenerator(rating_df, n_test=15)\n",
    "\n",
    "# Get train and test dataframes\n",
    "df1, df2 = sample_generator.get_train_test_dataframes()\n",
    "\n",
    "print(df1.shape, df2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(909609, 2)\n",
      "(90600, 2)\n",
      "Train Size: 909609, Val Size: 90600\n"
     ]
    }
   ],
   "source": [
    "# now create a new dataframe columns\n",
    "#df['prompt'] = What star rating do you think userid_df[user_id] will give item_df[item_id]? \n",
    "#df['rating'] = df[rating]\n",
    "\n",
    "train_df = pd.DataFrame()\n",
    "train_df['prompt'] = \"What star rating do you think user_\" + df1['userId'].astype(str) + \" will give item_\" + df1['itemId'].astype(str) + \"?\"\n",
    "train_df['rating'] = df1['rating']\n",
    "print(train_df.shape)   \n",
    "\n",
    "val_df = pd.DataFrame()\n",
    "val_df['prompt'] = \"What star rating do you think user_\" + df2['userId'].astype(str) + \" will give item_\" + df2['itemId'].astype(str) + \"?\"\n",
    "val_df['rating'] = df2['rating']\n",
    "print(val_df.shape)\n",
    "\n",
    "print(f\"Train Size: {len(train_df)}, Val Size: {len(val_df)}\")\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from dataclasses import dataclass\n",
    "from transformers import GPT2LMHeadModel, GPT2ForSequenceClassification\n",
    "\n",
    "\n",
    "################################################################################\n",
    "# Transformer Model classes with Mamba layers\n",
    "################################################################################\n",
    "\n",
    "@dataclass\n",
    "class MambaConfig:\n",
    "    block_size: int = 1024\n",
    "    vocab_size: int = 50304\n",
    "    n_layer: int = 12\n",
    "    n_head: int = 12\n",
    "    n_embd: int = 768\n",
    "    dropout: float = 0.0\n",
    "    bias: bool = True\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    \"\"\" LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False \"\"\"\n",
    "    def __init__(self, ndim, bias):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(ndim))\n",
    "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
    "\n",
    "    def forward(self, input):\n",
    "        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
    "        self.gelu    = nn.GELU()\n",
    "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)   \n",
    "        return x\n",
    "\n",
    "class Mamba_custom(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model,\n",
    "        d_state=16,\n",
    "        d_conv=4,\n",
    "        expand=2,\n",
    "        dt_rank=\"auto\",\n",
    "        dt_min=0.001,\n",
    "        dt_max=0.1,\n",
    "        dt_init=\"random\",\n",
    "        dt_scale=1.0,\n",
    "        dt_init_floor=1e-4,\n",
    "        conv_bias=True,\n",
    "        bias=False,\n",
    "        use_fast_path=True,  # Fused kernel options\n",
    "        layer_idx=None,\n",
    "        device=None,\n",
    "        dtype=None,\n",
    "    ):\n",
    "        factory_kwargs = {\"device\": device, \"dtype\": dtype}\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_state = d_state\n",
    "        self.d_conv = d_conv\n",
    "        self.expand = expand\n",
    "        self.d_inner = int(self.expand * self.d_model)\n",
    "        self.dt_rank = math.ceil(self.d_model / 16) if dt_rank == \"auto\" else dt_rank\n",
    "        self.use_fast_path = use_fast_path\n",
    "        self.layer_idx = layer_idx\n",
    "\n",
    "        self.in_proj = nn.Linear(self.d_model, self.d_inner * 2, bias=bias, **factory_kwargs)\n",
    "\n",
    "        self.conv1d = nn.Conv1d(\n",
    "            in_channels=self.d_inner,\n",
    "            out_channels=self.d_inner,\n",
    "            bias=conv_bias,\n",
    "            kernel_size=d_conv,\n",
    "            groups=self.d_inner,\n",
    "            padding=d_conv - 1,\n",
    "            **factory_kwargs,\n",
    "        )\n",
    "\n",
    "        self.activation = \"silu\"\n",
    "        self.act = nn.SiLU()\n",
    "\n",
    "        self.x_proj = nn.Linear(\n",
    "            self.d_inner, self.dt_rank + self.d_state * 2, bias=False, **factory_kwargs\n",
    "        )\n",
    "        self.dt_proj = nn.Linear(self.dt_rank, self.d_inner, bias=True, **factory_kwargs)\n",
    "\n",
    "        # Initialize special dt projection to preserve variance at initialization\n",
    "        dt_init_std = self.dt_rank**-0.5 * dt_scale\n",
    "        if dt_init == \"constant\":\n",
    "            nn.init.constant_(self.dt_proj.weight, dt_init_std)\n",
    "        elif dt_init == \"random\":\n",
    "            nn.init.uniform_(self.dt_proj.weight, -dt_init_std, dt_init_std)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        # Initialize dt bias so that F.softplus(dt_bias) is between dt_min and dt_max\n",
    "        dt = torch.exp(\n",
    "            torch.rand(self.d_inner, **factory_kwargs) * (math.log(dt_max) - math.log(dt_min))\n",
    "            + math.log(dt_min)\n",
    "        ).clamp(min=dt_init_floor)\n",
    "        # Inverse of softplus: https://github.com/pytorch/pytorch/issues/72759\n",
    "        inv_dt = dt + torch.log(-torch.expm1(-dt))\n",
    "        with torch.no_grad():\n",
    "            self.dt_proj.bias.copy_(inv_dt)\n",
    "        # Our initialization would set all Linear.bias to zero, need to mark this one as _no_reinit\n",
    "        self.dt_proj.bias._no_reinit = True\n",
    "\n",
    "        # S4D real initialization\n",
    "        A = repeat(\n",
    "            torch.arange(1, self.d_state + 1, dtype=torch.float32, device=device),\n",
    "            \"n -> d n\",\n",
    "            d=self.d_inner,\n",
    "        ).contiguous()\n",
    "        A_log = torch.log(A)  # Keep A_log in fp32\n",
    "        self.A_log = nn.Parameter(A_log)\n",
    "        self.A_log._no_weight_decay = True\n",
    "\n",
    "        # D \"skip\" parameter\n",
    "        self.D = nn.Parameter(torch.ones(self.d_inner, device=device))  # Keep in fp32\n",
    "        self.D._no_weight_decay = True\n",
    "\n",
    "        self.out_proj = nn.Linear(self.d_inner, self.d_model, bias=bias, **factory_kwargs)\n",
    "\n",
    "    def forward(self, hidden_states, inference_params=None):\n",
    "        \"\"\"\n",
    "        hidden_states: (B, L, D)\n",
    "        Returns: same shape as hidden_states\n",
    "        \"\"\"\n",
    "        batch, seqlen, dim = hidden_states.shape\n",
    "\n",
    "        conv_state, ssm_state = None, None\n",
    "        if inference_params is not None:\n",
    "            conv_state, ssm_state = self._get_states_from_cache(inference_params, batch)\n",
    "            if inference_params.seqlen_offset > 0:\n",
    "                # The states are updated inplace\n",
    "                out, _, _ = self.step(hidden_states, conv_state, ssm_state)\n",
    "                return out\n",
    "\n",
    "        # We do matmul and transpose BLH -> HBL at the same time\n",
    "        xz = rearrange(\n",
    "            self.in_proj.weight @ rearrange(hidden_states, \"b l d -> d (b l)\"),\n",
    "            \"d (b l) -> b d l\",\n",
    "            l=seqlen,\n",
    "        )\n",
    "        if self.in_proj.bias is not None:\n",
    "            xz = xz + rearrange(self.in_proj.bias.to(dtype=xz.dtype), \"d -> d 1\")\n",
    "\n",
    "        A = -torch.exp(self.A_log.float())  # (d_inner, d_state)\n",
    "        # In the backward pass we write dx and dz next to each other to avoid torch.cat\n",
    "        if self.use_fast_path and causal_conv1d_fn is not None and inference_params is None:  # Doesn't support outputting the states\n",
    "            out = mamba_inner_fn(\n",
    "                xz,\n",
    "                self.conv1d.weight,\n",
    "                self.conv1d.bias,\n",
    "                self.x_proj.weight,\n",
    "                self.dt_proj.weight,\n",
    "                self.out_proj.weight,\n",
    "                self.out_proj.bias,\n",
    "                A,\n",
    "                None,  # input-dependent B\n",
    "                None,  # input-dependent C\n",
    "                self.D.float(),\n",
    "                delta_bias=self.dt_proj.bias.float(),\n",
    "                delta_softplus=True,\n",
    "            )\n",
    "        else:\n",
    "            x, z = xz.chunk(2, dim=1)\n",
    "            # Compute short convolution\n",
    "            if conv_state is not None:\n",
    "                # If we just take x[:, :, -self.d_conv :], it will error if seqlen < self.d_conv\n",
    "                # Instead F.pad will pad with zeros if seqlen < self.d_conv, and truncate otherwise.\n",
    "                conv_state.copy_(F.pad(x, (self.d_conv - x.shape[-1], 0)))  # Update state (B D W)\n",
    "            if causal_conv1d_fn is None:\n",
    "                x = self.act(self.conv1d(x)[..., :seqlen])\n",
    "            else:\n",
    "                assert self.activation in [\"silu\", \"swish\"]\n",
    "                x = causal_conv1d_fn(\n",
    "                    x=x,\n",
    "                    weight=rearrange(self.conv1d.weight, \"d 1 w -> d w\"),\n",
    "                    bias=self.conv1d.bias,\n",
    "                    activation=self.activation,\n",
    "                )\n",
    "\n",
    "            # We're careful here about the layout, to avoid extra transposes.\n",
    "            # We want dt to have d as the slowest moving dimension\n",
    "            # and L as the fastest moving dimension, since those are what the ssm_scan kernel expects.\n",
    "            x_dbl = self.x_proj(rearrange(x, \"b d l -> (b l) d\"))  # (bl d)\n",
    "            dt, B, C = torch.split(x_dbl, [self.dt_rank, self.d_state, self.d_state], dim=-1)\n",
    "            dt = self.dt_proj.weight @ dt.t()\n",
    "            dt = rearrange(dt, \"d (b l) -> b d l\", l=seqlen)\n",
    "            B = rearrange(B, \"(b l) dstate -> b dstate l\", l=seqlen).contiguous()\n",
    "            C = rearrange(C, \"(b l) dstate -> b dstate l\", l=seqlen).contiguous()\n",
    "            assert self.activation in [\"silu\", \"swish\"]\n",
    "            y = selective_scan_fn(\n",
    "                x,\n",
    "                dt,\n",
    "                A,\n",
    "                B,\n",
    "                C,\n",
    "                self.D.float(),\n",
    "                z=z,\n",
    "                delta_bias=self.dt_proj.bias.float(),\n",
    "                delta_softplus=True,\n",
    "                return_last_state=ssm_state is not None,\n",
    "            )\n",
    "            if ssm_state is not None:\n",
    "                y, last_state = y\n",
    "                ssm_state.copy_(last_state)\n",
    "            y = rearrange(y, \"b d l -> b l d\")\n",
    "            out = self.out_proj(y)\n",
    "        return out\n",
    "\n",
    "    def step(self, hidden_states, conv_state, ssm_state):\n",
    "        dtype = hidden_states.dtype\n",
    "        assert hidden_states.shape[1] == 1, \"Only support decoding with 1 token at a time for now\"\n",
    "        xz = self.in_proj(hidden_states.squeeze(1))  # (B 2D)\n",
    "        x, z = xz.chunk(2, dim=-1)  # (B D)\n",
    "\n",
    "        # Conv step\n",
    "        if causal_conv1d_update is None:\n",
    "            conv_state.copy_(torch.roll(conv_state, shifts=-1, dims=-1))  # Update state (B D W)\n",
    "            conv_state[:, :, -1] = x\n",
    "            x = torch.sum(conv_state * rearrange(self.conv1d.weight, \"d 1 w -> d w\"), dim=-1)  # (B D)\n",
    "            if self.conv1d.bias is not None:\n",
    "                x = x + self.conv1d.bias\n",
    "            x = self.act(x).to(dtype=dtype)\n",
    "        else:\n",
    "            x = causal_conv1d_update(\n",
    "                x,\n",
    "                conv_state,\n",
    "                rearrange(self.conv1d.weight, \"d 1 w -> d w\"),\n",
    "                self.conv1d.bias,\n",
    "                self.activation,\n",
    "            )\n",
    "\n",
    "        x_db = self.x_proj(x)  # (B dt_rank+2*d_state)\n",
    "        dt, B, C = torch.split(x_db, [self.dt_rank, self.d_state, self.d_state], dim=-1)\n",
    "        # Don't add dt_bias here\n",
    "        dt = F.linear(dt, self.dt_proj.weight)  # (B d_inner)\n",
    "        A = -torch.exp(self.A_log.float())  # (d_inner, d_state)\n",
    "\n",
    "        # SSM step\n",
    "        if selective_state_update is None:\n",
    "            # Discretize A and B\n",
    "            dt = F.softplus(dt + self.dt_proj.bias.to(dtype=dt.dtype))\n",
    "            dA = torch.exp(torch.einsum(\"bd,dn->bdn\", dt, A))\n",
    "            dB = torch.einsum(\"bd,bn->bdn\", dt, B)\n",
    "            ssm_state.copy_(ssm_state * dA + rearrange(x, \"b d -> b d 1\") * dB)\n",
    "            y = torch.einsum(\"bdn,bn->bd\", ssm_state.to(dtype), C)\n",
    "            y = y + self.D.to(dtype) * x\n",
    "            y = y * self.act(z)  # (B D)\n",
    "        else:\n",
    "            y = selective_state_update(\n",
    "                ssm_state, x, dt, A, B, C, self.D, z=z, dt_bias=self.dt_proj.bias, dt_softplus=True\n",
    "            )\n",
    "\n",
    "        out = self.out_proj(y)\n",
    "        return out.unsqueeze(1), conv_state, ssm_state\n",
    "\n",
    "    def allocate_inference_cache(self, batch_size, max_seqlen, dtype=None, **kwargs):\n",
    "        device = self.out_proj.weight.device\n",
    "        conv_dtype = self.conv1d.weight.dtype if dtype is None else dtype\n",
    "        conv_state = torch.zeros(\n",
    "            batch_size, self.d_model * self.expand, self.d_conv, device=device, dtype=conv_dtype\n",
    "        )\n",
    "        ssm_dtype = self.dt_proj.weight.dtype if dtype is None else dtype\n",
    "        # ssm_dtype = torch.float32\n",
    "        ssm_state = torch.zeros(\n",
    "            batch_size, self.d_model * self.expand, self.d_state, device=device, dtype=ssm_dtype\n",
    "        )\n",
    "        return conv_state, ssm_state\n",
    "\n",
    "    def _get_states_from_cache(self, inference_params, batch_size, initialize_states=False):\n",
    "        assert self.layer_idx is not None\n",
    "        if self.layer_idx not in inference_params.key_value_memory_dict:\n",
    "            batch_shape = (batch_size,)\n",
    "            conv_state = torch.zeros(\n",
    "                batch_size,\n",
    "                self.d_model * self.expand,\n",
    "                self.d_conv,\n",
    "                device=self.conv1d.weight.device,\n",
    "                dtype=self.conv1d.weight.dtype,\n",
    "            )\n",
    "            ssm_state = torch.zeros(\n",
    "                batch_size,\n",
    "                self.d_model * self.expand,\n",
    "                self.d_state,\n",
    "                device=self.dt_proj.weight.device,\n",
    "                dtype=self.dt_proj.weight.dtype,\n",
    "                # dtype=torch.float32,\n",
    "            )\n",
    "            inference_params.key_value_memory_dict[self.layer_idx] = (conv_state, ssm_state)\n",
    "        else:\n",
    "            conv_state, ssm_state = inference_params.key_value_memory_dict[self.layer_idx]\n",
    "            # TODO: What if batch size changes between generation, and we reuse the same states?\n",
    "            if initialize_states:\n",
    "                conv_state.zero_()\n",
    "                ssm_state.zero_()\n",
    "        return conv_state, ssm_state\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.attn = Mamba_custom(config.n_embd)\n",
    "        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class MAMBA(nn.Module):\n",
    "    def __init__(self, model_type='mamba', is_gen=False):\n",
    "        super(MAMBA, self).__init__()\n",
    "        \n",
    "        # n_layer and n_embd are determined from model_type\n",
    "        config_args = {\n",
    "            'mamba':         dict(n_layer=12,n_embd=768),  # 124M params\n",
    "            'mamba-small':  dict(n_layer=6,  n_embd=768), # 350M params\n",
    "            'mamba-xsmal':   dict(n_layer=3, n_embd=768), # 774M params\n",
    "            'mamba-singlelayer': dict(n_layer=1,n_embd=768), # 1558M (1.3B) params\n",
    "        }[model_type]\n",
    "        print(\"forcing vocab_size=50257, block_size=1024, bias=True\")\n",
    "        config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints\n",
    "        config_args['block_size'] = 1024  # always 1024 for GPT model checkpoints\n",
    "        config_args['bias'] = True        # always True for GPT model checkpoints\n",
    "        \n",
    "\n",
    "        self.config = MambaConfig(**config_args)\n",
    "        \n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(self.config.vocab_size, self.config.n_embd),\n",
    "            wpe = nn.Embedding(self.config.block_size, self.config.n_embd),\n",
    "            drop = nn.Dropout(self.config.dropout),\n",
    "            h = nn.ModuleList([Block(self.config) for _ in range(self.config.n_layer)]),\n",
    "            ln_f = LayerNorm(self.config.n_embd, bias=self.config.bias),\n",
    "        ))\n",
    "        if is_gen:\n",
    "            self.lm_head = nn.Linear(\n",
    "                self.config.n_embd, self.config.vocab_size, bias=False)\n",
    "            self.transformer.wte.weight = self.lm_head.weight  # https://paperswithcode.com/method/weight-tying\n",
    "            self.score = None\n",
    "        else:\n",
    "            self.score = nn.Linear(self.config.n_embd, 5, bias=False)\n",
    "\n",
    "        # TODO: Remove gradients from the embedding layers\n",
    "\n",
    "        # added by me below\n",
    "        # self.transformer.wte.weight.requires_grad = False\n",
    "        # self.transformer.wpe.weight.requires_grad = False\n",
    "        # added by me above \n",
    "\n",
    "        # Initialize weights with Xavier initialization\\n\",\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "        # print the total number of parameters\n",
    "        total_params = sum(p.numel() for p in self.parameters())\n",
    "        print(f\"Number of parameters: {total_params / 1e6:.2f}M\")\n",
    "        if not is_gen:\n",
    "            # print the number of trainable parameters\n",
    "            num_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "            print(f\"Number of trainable parameters: {num_params / 1e6:.2f}M\")\n",
    "            # calculate the reduction in parameters\n",
    "            reduction = 100 * (total_params - num_params) / total_params\n",
    "            print(f\"Reduction: {reduction:.2f}%\")\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "    # Check if the module is either a Linear or Embedding layer\n",
    "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "            # Apply Xavier (Glorot) uniform initialization to the weights\n",
    "            init.xavier_uniform_(module.weight)\n",
    "            \n",
    "            # If the module has a bias, initialize it to zero\n",
    "            if hasattr(module, 'bias') and module.bias is not None:\n",
    "                module.bias.data.fill_(0)\n",
    "\n",
    "        # Check if the module is a LayerNorm\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            # Set the bias to zero and the weight to 1 for LayerNorm\n",
    "            module.bias.data.fill_(0)\n",
    "            module.weight.data.fill_(1.0)\n",
    "        \n",
    "        # If the module is a ModuleList, recursively initialize its weights\n",
    "        elif isinstance(module, nn.ModuleList):\n",
    "            for submodule in module:\n",
    "                self._init_weights(submodule)\n",
    "\n",
    "\n",
    "    def forward(self, idx, mask=None):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device) # shape (t)\n",
    "\n",
    "        # if mask is provided, find the indices of the last tokens in each sequence\n",
    "        if mask is not None:\n",
    "            assert mask.size() == idx.size(), \"Mask size must match input size\"\n",
    "            eos_idxs = mask.sum(1) - 1 # last non-pad token in each sequence\n",
    "\n",
    "        # forward the GPT model itself\n",
    "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
    "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (t, n_embd)\n",
    "        x = self.transformer.drop(tok_emb + pos_emb)\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "\n",
    "        if self.score is None:\n",
    "            # inference-time mini-optimization: only forward the lm_head on the very last position\n",
    "            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n",
    "        else: # no need to preserve the time dimension for classification task\n",
    "            if mask is not None:\n",
    "                # if mask is provided, only return the logits for the last token in each sequence\n",
    "                logits = self.score(x[torch.arange(b, device=device), eos_idxs])\n",
    "            else:\n",
    "                \n",
    "                logits = self.score(x[:, -1, :])\n",
    "               \n",
    "        # return logits.squeeze(-1)\n",
    "        return logits\n",
    "\n",
    "    def crop_block_size(self, block_size):\n",
    "        # model surgery to decrease the block size if necessary\n",
    "        # e.g. we may load the GPT2 pretrained model checkpoint (block size 1024)\n",
    "        # but want to use a smaller block size for some smaller, simpler model\n",
    "        assert block_size <= self.config.block_size\n",
    "        self.config.block_size = block_size\n",
    "        self.transformer.wpe.weight = nn.Parameter(self.transformer.wpe.weight[:block_size])\n",
    "        for block in self.transformer.h:\n",
    "            if hasattr(block.attn, 'bias'):\n",
    "                block.attn.bias = block.attn.bias[:,:,:block_size,:block_size]\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        \"\"\"\n",
    "        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
    "        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
    "        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
    "        \"\"\"\n",
    "        \n",
    "        for _ in range(max_new_tokens):\n",
    "            # if the sequence context is growing too long we must crop it at block_size\n",
    "            \n",
    "            # print(f\"shape of idx in generate: {idx.shape}\") # added by me \n",
    "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
    "            # forward the model to get the logits for the index in the sequence\n",
    "            logits = self(idx_cond)\n",
    "            # pluck the logits at the final step and scale by desired temperature\n",
    "            \n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            # optionally crop the logits to only the top k options\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "            # apply softmax to convert logits to (normalized) probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            # append sampled index to the running sequence and continue\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n",
    "    \n",
    "    def save_trainable_params(self, path):\n",
    "        trainable_params =\\\n",
    "            list(filter(lambda p: p.requires_grad, self.parameters()))\n",
    "        torch.save(trainable_params, path)\n",
    "    \n",
    "    def load_trainable_params(self, path):\n",
    "        trainable_params = torch.load(path)\n",
    "        for name, param in self.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                param.data = trainable_params.pop(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loader(data, batch_size, tokenizer, shuffle=True, max_len=30):\n",
    "    \"\"\"\n",
    "    Get a data loader for the training data.\n",
    "    \"\"\"\n",
    "    X, y = data['prompt'], data['rating']\n",
    "    y= y-1\n",
    "    X = tokenizer.batch_encode_plus(\n",
    "        X.tolist(), max_length=max_len, truncation=True, padding='max_length')\n",
    "    X, mask = X['input_ids'], X['attention_mask']\n",
    "    # convert them to tensors\n",
    "    X = torch.tensor(X)\n",
    "    mask = torch.tensor(mask)\n",
    "    y = torch.tensor(y.values, dtype=int)\n",
    "    data = torch.utils.data.TensorDataset(X, mask, y)\n",
    "    \n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        data, batch_size=batch_size, shuffle=shuffle)\n",
    "    \n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def train(model, train_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_pred = 0\n",
    "    total_pred = 0\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        X, mask, y = batch\n",
    "        X, mask, y = X.to(device), mask.to(device), y.to(device)\n",
    "        output = model(X, mask) # return logits for last valid token\n",
    "        \n",
    "        loss = criterion(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        correct_pred += (predicted == y).sum().item()\n",
    "        total_pred += len(y)\n",
    "    \n",
    "    return total_loss / len(train_loader), correct_pred / total_pred \n",
    "  \n",
    "def evaluate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct_pred = 0\n",
    "    total_pred = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            X, mask, y = batch\n",
    "            X, mask, y = X.to(device), mask.to(device), y.to(device)\n",
    "            output = model(X, mask)\n",
    "            loss = criterion(output, y)\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            correct_pred += (predicted == y).sum().item()\n",
    "            total_pred += len(y)\n",
    "             \n",
    "    return total_loss / len(val_loader) , correct_pred / total_pred \n",
    "\n",
    "    \n",
    "def plot_losses(train_losses, val_losses, mode, args):\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(val_losses, label='Val Loss')\n",
    "    plt.title(f'{mode}_{args.mamba_variant} Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'../plots1m/{args.mamba_variant}_loss.png')\n",
    "    plt.close()\n",
    "    print(f\"Plots saved at plots/{args.mamba_variant}_loss.png\")\n",
    "    \n",
    "def plot_metrics(train_accs, val_accs, mode, args):\n",
    "    plt.plot(train_accs, label='Train Acc')\n",
    "    plt.plot(val_accs, label='Val Acc')\n",
    "    plt.title(f'{mode} Accuracy')\n",
    "    plt.xlabel('Epoch') \n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'../plots1m/{args.mamba_variant}_acc.png')\n",
    "    plt.close()\n",
    "    print(f\"Plots saved at plots/{args.mamba_variant}_acc.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train_loader:  3554\n",
      "Length of val_loader:  354\n"
     ]
    }
   ],
   "source": [
    "train_loader = get_data_loader(train_df, 256, tokenizer, shuffle=True)\n",
    "val_loader = get_data_loader(val_df, 256, tokenizer, shuffle=False)\n",
    "\n",
    "print(\"Length of train_loader: \", len(train_loader))\n",
    "print(\"Length of val_loader: \", len(val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description=\"Mamba Model Training and Generation\")\n",
    "    parser.add_argument(\"--mode\", type=str, default=\"mamba\", choices=[\"mamba\", \"gen\"], help=\"Mode of operation: 'mamba' for training, 'gen' for generation\")\n",
    "    parser.add_argument(\"--gpu_id\", type=int, default=3, help=\"GPU ID to use\")\n",
    "    parser.add_argument(\"--mamba_variant\", type=str, default=\"mamba-singlelayer\", choices=[\"mamba\", \"mamba-small\", \"mamba-xsmall\", \"mamba-singlelayer\"], help=\"Variant of the Mamba model\")\n",
    "    parser.add_argument(\"--max_new_tokens\", type=int, default=100, help=\"Maximum number of new tokens to generate\")\n",
    "    parser.add_argument(\"--model_path\", type=str, default=\"models/mamba.pth\", help=\"Path to save the trained model\")\n",
    "    parser.add_argument(\"--lr\", type=float, default=1e-3, help=\"Learning rate\")\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=64, help=\"Batch size\")\n",
    "    parser.add_argument(\"--epochs\", type=int, default=2, help=\"Number of epochs\")\n",
    "    return parser.parse_args()\n",
    "\n",
    "args = parse_args()\n",
    "args.device = torch.device(\n",
    "    f\"cuda:{args.gpu_id}\" if torch.cuda.is_available() and args.gpu_id >= 0 else\\\n",
    "    \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "def main(args):\n",
    "    if args.mode == \"gen\":\n",
    "        model = MAMBA(args.mamba_variant, is_gen=True).to(args.device)\n",
    "        model.eval()\n",
    "\n",
    "        # TODO: You can add your super creative prompt here\n",
    "        prompt = \"My name is Inigo Montoya. You killed my father. Prepare to die. \"\n",
    "        # prompt = \"Once upon a time, in a land far far away, there was a\"\n",
    "\n",
    "        input_ids = tokenizer.encode(prompt, return_tensors='pt').to(args.device)\n",
    "        output = model.generate(input_ids, max_new_tokens=args.max_new_tokens)\n",
    "        print(\"\", tokenizer.decode(output[0]), sep=\"\\n\")\n",
    "\n",
    "    elif args.mode == \"mamba\":    \n",
    "        model = MAMBA(args.mamba_variant).to(args.device)\n",
    "        \n",
    "        # TODO: Implement the training loop (fill the train and evaluate functions in train_utils.py)\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        train_accs = []\n",
    "        val_accs = []\n",
    "        \n",
    "        for epoch in range(args.epochs):\n",
    "            # i want some seperation between print statements of epochs\n",
    "            print('-' * 80)\n",
    "            epoch_start = time.time()\n",
    "\n",
    "            train_start= time.time()\n",
    "            train_loss,train_acc = train(model, train_loader, optimizer, criterion, args.device)\n",
    "            train_end= time.time()\n",
    "\n",
    "            val_start= time.time()\n",
    "            val_loss,val_acc = evaluate(model, val_loader, criterion, args.device)\n",
    "            val_end= time.time()\n",
    "\n",
    "            epoch_end = time.time()\n",
    "\n",
    "            # Calculate the durations\n",
    "            train_duration = train_end - train_start\n",
    "            val_duration = val_end - val_start\n",
    "            epoch_duration = epoch_end - epoch_start\n",
    "    \n",
    "            train_losses.append(train_loss)\n",
    "            val_losses.append(val_loss)\n",
    "            train_accs.append(train_acc)\n",
    "            val_accs.append(val_acc)\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}/{args.epochs}: Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "            print(f\"Time taken for epoch {epoch+1}: {epoch_duration:.4f} seconds (Training: {train_duration:.4f} seconds, Evaluation: {val_duration:.4f} seconds)\")\n",
    "            # save the model after every 10 epochs in model directory with different names use save_trainable_params\n",
    "        #     if (epoch) % 25 == 0:\n",
    "        #         model.save_trainable_params(f'models/{args.mamba_variant}_{epoch+1}.pth')\n",
    "        # # save the train_losses and val_losses lists in some file so that we can plot them later\n",
    "        with open(f'../results_food/{args.mamba_variant}_metrics.txt', 'w') as f:\n",
    "            f.write('Train Losses:\\n')\n",
    "            f.write(str(train_losses))\n",
    "            f.write('\\nVal Losses:\\n')\n",
    "            f.write(str(val_losses))\n",
    "            f.write('\\nTrain Accuracies:\\n')\n",
    "            f.write(str(train_accs))\n",
    "            f.write('\\nVal Accuracies:\\n')\n",
    "            f.write(str(val_accs))\n",
    "        print(f\"Metrics saved at model/{args.mamba_variant}_metrics.txt\")\n",
    "        # save the plot in plots folder \n",
    "        plot_losses(train_losses, val_losses, args.mode, args)\n",
    "        plot_metrics(train_accs, val_accs, args.mode, args)\n",
    "        # model.save_trainable_params(args.model_path)\n",
    "    else:\n",
    "        print(\"Invalid mode\")\n",
    "        return\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forcing vocab_size=50257, block_size=1024, bias=True\n",
      "Number of parameters: 141.34M\n",
      "Number of trainable parameters: 141.34M\n",
      "Reduction: 0.00%\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 1/2:Train RMSE: 4.3662, Val RMSE: 5.4500\n",
      "Time taken for epoch 1: 237.4135 seconds (Training: 226.5688 seconds, Evaluation: 10.8447 seconds)\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 2/2:Train RMSE: 7.0394, Val RMSE: 8.2115\n",
      "Time taken for epoch 2: 238.7160 seconds (Training: 227.8771 seconds, Evaluation: 10.8389 seconds)\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# from transformers import AutoTokenizer\n",
    "# import matplotlib.pyplot as plt\n",
    "# import os\n",
    "\n",
    "\n",
    "# # Define the arguments in a dictionary\n",
    "# args = {\n",
    "#     \"mode\": \"mamba\",  \n",
    "#     \"gpu_id\": 3,\n",
    "#     \"mamba_variant\": \"mamba\", # Change this to \"mamba\", \"mamba-small\", \"mamba-xsmall\", or \"mamba-singlelayer\"\n",
    "#     \"max_new_tokens\": 100,\n",
    "#     \"model_path\": \"models/mamba.pth\",\n",
    "#     \"lr\": 1e-3,\n",
    "#     \"batch_size\": 64,\n",
    "#     \"epochs\": 2\n",
    "# }\n",
    "\n",
    "# # Convert the dictionary to an object with attributes\n",
    "# class Args:\n",
    "#     def __init__(self, **entries):\n",
    "#         self.__dict__.update(entries)\n",
    "\n",
    "# args = Args(**args)\n",
    "# args.device = torch.device(\n",
    "#     \"cuda:6\" if torch.cuda.is_available() and args.gpu_id >= 0 else\\\n",
    "#     \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# # Define the main function and other necessary functions\n",
    "# def main(args):\n",
    "#     if args.mode == \"gen\":\n",
    "#         model = MAMBA(args.mamba_variant, is_gen=True).to(args.device)\n",
    "#         model.eval()\n",
    "\n",
    "#         # TODO: You can add your super creative prompt here\n",
    "#         prompt = \"My name is Inigo Montoya. You killed my father. Prepare to die. \"\n",
    "#         # prompt = \"Once upon a time, in a land far far away, there was a\"\n",
    "\n",
    "#         input_ids = tokenizer.encode(prompt, return_tensors='pt').to(args.device)\n",
    "#         output = model.generate(input_ids, max_new_tokens=args.max_new_tokens)\n",
    "#         print(\"\", tokenizer.decode(output[0]), sep=\"\\n\")\n",
    "\n",
    "#     elif args.mode == \"mamba\":    \n",
    "#         model = MAMBA(args.mamba_variant).to(args.device)\n",
    "        \n",
    "#         # TODO: Implement the training loop (fill the train and evaluate functions in train_utils.py)\n",
    "#         criterion = torch.nn.CrossEntropyLoss()\n",
    "#         optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "#         train_losses = []\n",
    "#         val_losses = []\n",
    "#         train_rmses = []\n",
    "#         val_rmses = []        \n",
    "#         for epoch in range(args.epochs):\n",
    "#             print('-' * 80)\n",
    "#             epoch_start = time.time()\n",
    "\n",
    "#             # Training\n",
    "#             train_start = time.time()\n",
    "#             train_loss, train_rmse = train(model, train_loader, optimizer, criterion, args.device)\n",
    "#             train_end = time.time()\n",
    "\n",
    "#             # Validation\n",
    "#             val_start = time.time()\n",
    "#             val_loss, val_rmse = evaluate(model, val_loader, criterion, args.device)\n",
    "#             val_end = time.time()\n",
    "\n",
    "#             epoch_end = time.time()\n",
    "\n",
    "#             # Calculate durations\n",
    "#             train_duration = train_end - train_start\n",
    "#             val_duration = val_end - val_start\n",
    "#             epoch_duration = epoch_end - epoch_start\n",
    "\n",
    "#             # Append metrics\n",
    "#             train_losses.append(train_loss)\n",
    "#             val_losses.append(val_loss)\n",
    "#             train_rmses.append(train_rmse)\n",
    "#             val_rmses.append(val_rmse)\n",
    "\n",
    "#             # Print metrics for the current epoch\n",
    "#             print(f\"Epoch {epoch+1}/{args.epochs}:Train RMSE: {train_rmse:.4f}, \"\n",
    "#                   f\"Val RMSE: {val_rmse:.4f}\")\n",
    "#             print(f\"Time taken for epoch {epoch+1}: {epoch_duration:.4f} seconds \"\n",
    "#                   f\"(Training: {train_duration:.4f} seconds, Evaluation: {val_duration:.4f} seconds)\")\n",
    "            \n",
    "        \n",
    "#         # TODO: Also plot the training losses and metrics\n",
    "#         # save the plot in plots folder \n",
    "#         # plot_losses(train_losses, val_losses, args.mode, args)\n",
    "#         # plot_metrics(train_accs, val_accs, args.mode, args)\n",
    "#         # # model.save_trainable_params(args.model_path)\n",
    "#     else:\n",
    "#         print(\"Invalid mode\")\n",
    "#         return\n",
    "\n",
    "# # Call the main function\n",
    "# main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
